{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Presdigitator/abstract-generator/blob/main/Introduction_to_Deep_Learning_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Deep Learning in PyTorch**\n",
        "\n",
        "*QWER Hacks $\\times$ ACM AI*\n",
        "\n",
        "Jordan Lin & Jackson Steele"
      ],
      "metadata": {
        "id": "gm5lI1cv6EEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Deep Learning?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rr32TX08s4_M6LVR0HVO8ndr5QY2Q-Q7\" width=\"700\">\n",
        "\n",
        "### Some Key Terms\n",
        "\n",
        "*   **Artificial Intelligence**\n",
        "  - Refers to the general concept of a computer doing things that would normally require a human\n",
        "*   **Machine Learning**\n",
        "  - A subset of Artifical Intelligence\n",
        "  - Most 'Artificial Intelligence' you see referred to today is more precisely referring to Machine Learning\n",
        "  - A methodology by which a computer algorithm is able to modify itself given data (in other words, it is able to *learn*)\n",
        "* **Deep Learning**\n",
        "  - A subset of machine learning\n",
        "  - An approach to machine learning in which a neural network is used to better be able to learn/identify patterns\n",
        "* **Neural Networks**\n",
        "  - A model of thousands of weights in several layers\n",
        "  - Involves a lot of complicated math that we don't really need to worry about right now\n",
        "  - Theoretically supposed to approximate how a human brain works\n",
        "\n",
        "### Using Deep Learning\n",
        "\n",
        "#### Part 1: Big Data\n",
        "If you want to make a deep learning model, you need data, and a lot of it.  Your model is probably going to need thousands of different data points at a minimum if you want to have a high level of accuracy.  This is the data that your model needs in order to 'learn' - computers are a lot worse than humans at learning, so a task that could be taught to a human with just a few examples is going to need orders of magnitude more examples for your model to learn off of.  On top of having a lot of data, there's another very important consideration&mdash;\n",
        "\n",
        "#### Part 1.5: Avoiding Bias/Unintended Patterns\n",
        "\n",
        "Deep learning models and techniques are designed to be very good at pattern recognition.  This means your model is probably going to pick up on any patterns present in the data, intentional or not.  This means we need to avoid any bias/patterns that could confuse our model.  For example, if we were training an image recognition model to distinguish between cats and dogs, and most of our cat pictures were taken inside while most of our dog pictures were taken outside, the model would probably struggle to correctly label dogs that were indoors.  So whenever you are deciding on what data to use, you want to try your best to avoid any unwanted patterns.\n",
        "\n",
        "#### Part 2: Understanding our Data\n",
        "\n",
        "Now that you have your data, you need to come up with a way for the model to understand your data.  There's a lot of fascinating techniques for this, and it can vary depending on what you're trying to do - to give an example, you can't load image data using a method designed for text.  Thankfully, most of the heavy lifting for this is built into pytorch (the library we are using), so you don't need to understand all the minutiae.\n",
        "\n",
        "#### Part 3: Making the Network\n",
        "\n",
        "You need to actually define the parameters of the neural network you would like to make.  You have to decide how many layers the network should have, how many nodes each layer should have, both of which can drastically effect the accuracy of the model.  Thankfully, once again pytorch does most of the heavy lifting here, so we can just tell it the parameters we would like and it will make the network for us.  We also need a loss function.  Essentially, our model has no way of knowing if it's doing good or bad, so we need to tell it how to calculate how good it's doing.  This is what the loss function is for.\n",
        "\n",
        "#### Part 4: Training\n",
        "\n",
        "Once we have everything set up, we can begin training our model!  This is mostly just a waiting game, but we do get to decide how long the model is going to train for.  In general, we don't want the model to train for too short a period of time, because then it won't have enough time to get accurate.  But after a certain point, models tend to give diminishing returns.  There's usually a level of accuracy that once reached, is very hard to meaningfully improve upon, so training for longer won't really deliver that much in terms of performance.\n",
        "\n",
        "#### We're done!\n",
        "\n",
        "Now that we have a trained model, we're pretty much done!  This model can now be saved for future use, and hopefully it does a pretty good job at whatever we wanted it to."
      ],
      "metadata": {
        "id": "MP5r8m7h6Z42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Tutorial :)"
      ],
      "metadata": {
        "id": "LVMqsngFEyVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyG9vkYDZNP4"
      },
      "outputs": [],
      "source": [
        "# numpy: the library that deals with linear algebra calculations\n",
        "import numpy as np\n",
        "# (py)torch: our machine learning library!\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim, utils\n",
        "from torchvision import datasets, transforms\n",
        "# matplotlib: the library for making plots & displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "# tqdm: the library for displaying progress bars\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use GPU is GPU (CUDA) is available, use CPU otherwise\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "06rEsDHPZpsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "We are using CIFAR-10, which comprises of $50000$ training images and $10000$ testing images with dimensions $32 \\times 32 \\times 3$ (square image $32$ in width, $3$ for the RGB channels).\n",
        "\n",
        "You may see that we first download the images to store in a `Datasets` object, which stores the unprocessed images (and handles the downloading, etc.). Then, we turn the `Datasets` object into a `DataLoader` object, which gives us an iterator that allows us to iterate through the entire dataset one `batch_size` of images at a time and is very helpful for training."
      ],
      "metadata": {
        "id": "qG8qHNyzbRRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process images\n",
        "transform = transforms.Compose([transforms.ToTensor()])  # convert image to pytorch tensor\n",
        "# download images and create PyTorch Dataset & DataLoader objects\n",
        "\n",
        "##### YOUR CODE HERE #####\n",
        "\n",
        "##### END OF CODE #####"
      ],
      "metadata": {
        "id": "Yr6WNHevbNed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the CIFAR-10 classes, 0 for plane, 1 for car, 2 for bird, etc.\n",
        "classes = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
      ],
      "metadata": {
        "id": "2i4oeIgHbvIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the CIFAR-10 classes and what they look like.\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/cifar10.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "nZfeOcEmcDul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation to prevent overfitting\n",
        "data_augmentation = nn.Sequential(transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.RandomCrop((32, 32), padding=(4, 4)))\n",
        "data_augmentation = torch.jit.script(data_augmentation)  # scriptify to make augmentation run faster"
      ],
      "metadata": {
        "id": "_HMwnrOwhlAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1hbdLbIbee3dZYUseQdmjiSBMKXnMYaIg\" width=\"700\">"
      ],
      "metadata": {
        "id": "fjSLFy73cJtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the CNN network and define its layers\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # there are many different ways of defining a PyTorch model, nn.Sequential is just the most\n",
        "        # convenient one as everything is in one callable 'thing' and executes in sequence\n",
        "\n",
        "        ##### YOUR CODE HERE #####\n",
        "\n",
        "        ##### END OF CODE #####\n",
        "\n",
        "        # ... however, you can do something like:\n",
        "        # self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
        "        # self.relu1 = nn.ReLU()\n",
        "        # sekf.batch_norm1 = nn.BatchNorm2d(32)\n",
        "        # ...\n",
        "        # and then in the forward function sequentially pass x through these class variables\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Pass input x through the network to obtain an output\"\"\"\n",
        "        output = self.network(x)\n",
        "        return output\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Count the number of parameters (i.e., size of weights) in the network\"\"\"\n",
        "        parameters = self.parameters()\n",
        "        size = 0\n",
        "        for parameter in parameters:\n",
        "            size += np.prod(parameter.shape)\n",
        "        return size"
      ],
      "metadata": {
        "id": "f331da5rb0s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = CNN()\n",
        "network.to(device)  # if we are using GPU, put the network weights on the GPU"
      ],
      "metadata": {
        "id": "WsHz9D2NdS8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Network size: {network.size()}\")"
      ],
      "metadata": {
        "id": "X-pqwxBGd0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criterion and Optimizer\n",
        "\n",
        "The **criterion**, or loss function, or objective (this thing has a lot of names) takes the network output and labels (or ground truth, i.e., what the network is *supposed* to predict) and returns a single value, called the **loss**, that essentially *measures* and *quantifies* how well our network is performing. **The lower the loss, the better our model is doing.** Hence, we want to minimize the loss.\n",
        "\n",
        "The **optimizer** takes the gradients (gradient of the loss function w.r.t. the network parameters) and applies them to the network weights/parameters, i.e., nudging the network in the direction of the greatest descent for the loss to minimize it."
      ],
      "metadata": {
        "id": "nLAO_QYUeZPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.NAdam(network.parameters(), lr=0.0003, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "kG-SFI6Zd4md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "It is *bad practice* to not split the training set further into the *actual* training set and validation set, where we train on the new (slightly smaller) training set and tune our hyperparameters according to how the validation set is doing in terms of accuracy, etc.\n",
        "\n",
        "The reason we do so is because the testing set should be **unseen data**, and if we tune our hyperparameters based on its performance, even though the network was never directly trained on it, *we* are being affected by the results of its performance and *we* are the ones who tune the network, so the network is, in a way, still being affected by it.\n",
        "\n",
        "We are not using validation set here and just evaluating on the testing set, which is not the greatest. However, for demonstration sake this is just simpler :P"
      ],
      "metadata": {
        "id": "UP1-YErifXfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()  # we don't want to compute gradients here!\n",
        "def evaluate(loader, network, criterion):\n",
        "    network.eval()  # put network into evaluation mode (mostly for batch normalization)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)  # put inputs and labels on GPU (if it is available)\n",
        "        labels = labels.to(device)\n",
        "        outputs = network(inputs)  # pass inputs through network to get outputs\n",
        "        loss = criterion(outputs, labels)  # evaluate outputs with criterion to get loss\n",
        "        accuracy = (torch.max(outputs, dim=1)[1] == labels).to(torch.float32).mean()  # accuracy\n",
        "        losses.append(loss.cpu().numpy())\n",
        "        accuracies.append(accuracy.cpu().numpy())\n",
        "    return np.mean(losses), np.mean(accuracies)"
      ],
      "metadata": {
        "id": "DjuglhxxfV_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_max = 32  # you can increase this if you want (takes longer, but accuracy will be higher)\n",
        "results = {\"train losses\": [], \"train accuracies\": [], \"test losses\": [], \"test accuracies\": []}\n",
        "\n",
        "for epoch in tqdm(range(epoch_max)):\n",
        "    for inputs, labels in tqdm(train_loader, leave=False):\n",
        "\n",
        "        ##### YOUR CODE HERE #####\n",
        "\n",
        "        # put inputs and labels on GPU (if it is available)\n",
        "        # perform data augmentation on inputs\n",
        "        # put network into training mode (mostly for batch normalization)\n",
        "        # zero-out gradients\n",
        "        # pass inputs through network to get outputs\n",
        "        # evaluate outputs bwith criterion to get loss\n",
        "        # backpropagate through loss to compute gradients (loss w.r.t. weights)\n",
        "        # use gradients to perform optimization (e.g., NAdam)\n",
        "\n",
        "        ##### END OF CODE #####\n",
        "    \n",
        "    # after every epoch, evaluate results on the entire training & testing set (slow)\n",
        "    train_loss, train_accuracy = evaluate(train_loader, network, criterion)\n",
        "    test_loss, test_accuracy = evaluate(test_loader, network, criterion)\n",
        "    # store results\n",
        "    results[\"train losses\"].append(train_loss)\n",
        "    results[\"train accuracies\"].append(train_accuracy)\n",
        "    results[\"test losses\"].append(test_loss)\n",
        "    results[\"test accuracies\"].append(test_accuracy)\n",
        "    # print results so we can see how the network is doing\n",
        "    result = f\"{('[' + str(epoch + 1) + ']'):5s}   \"\\\n",
        "                f\"Train: {str(train_accuracy * 100):.6}% ({str(train_loss):.6})   \"\\\n",
        "                f\"Test: {str(test_accuracy * 100):.6}% ({str(test_loss):.6})\"\n",
        "    tqdm.write(result)"
      ],
      "metadata": {
        "id": "bNmA5Dq9gKH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Bit on Gradient Descent\n",
        "\n",
        "How is our neural network model actually **learning**?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bUGik_Ocl4lFwu2XxwmXgKTkM2PA3UUr\" width=\"700\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Sri75orf7VxjDo6S3K-h49cQMjZZ19Db\" width=\"700\">"
      ],
      "metadata": {
        "id": "MNxk0_pPE8Fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "88AVnLW2iYiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1, epoch_max + 1)\n",
        "# plot losses\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, results[\"train losses\"], label=\"train\")\n",
        "plt.plot(epochs, results[\"test losses\"], label=\"test\")\n",
        "plt.title(\"Network loss (lower is better)\", fontsize=16)\n",
        "plt.xlabel(\"Epochs\", fontsize=14)\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend()\n",
        "# plot accuracies\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, np.asarray(results[\"train accuracies\"]) * 100, label=\"train\")\n",
        "plt.plot(epochs, np.asarray(results[\"test accuracies\"]) * 100, label=\"test\")\n",
        "plt.title(\"Network accuracy (higher is better)\", fontsize=16)\n",
        "plt.xlabel(\"Epochs\", fontsize=14)\n",
        "plt.ylabel(\"Accuracy (%)\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend()\n",
        "# show plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u12A8OPgg7LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def display_prediction(network, loader, i=0):\n",
        "    inputs, labels = next(iter(loader))\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    network.eval()\n",
        "    outputs = network(inputs)\n",
        "    predictions = outputs.argmax(dim=-1)\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(inputs[i].moveaxis(-3, -1).cpu())\n",
        "    print(f\"Predicted label: {classes[predictions[i].cpu().numpy()]} \"\n",
        "          f\"({predictions[i].cpu().numpy()})\")\n",
        "    print(f\"Actual label: {classes[labels[i].cpu().numpy()]} ({labels[i].cpu().numpy()})\")"
      ],
      "metadata": {
        "id": "j0EcY6jC7CIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_prediction(network, train_loader)"
      ],
      "metadata": {
        "id": "BXyhchT-95lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Weights\n",
        "\n",
        "We can save our weights as a file to be loaded later if we have our network definition set to the same thing. This can be very practical if you pre-train a model and want to deploy it in your projects, e.g., on the web."
      ],
      "metadata": {
        "id": "jHJ8R7ODihJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can put whatever extension you want, .pth (or .pt) is just the PyTorch convention\n",
        "torch.save(network.state_dict(), \"cifar10_model.pth\")"
      ],
      "metadata": {
        "id": "aFGGiR5uibtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Weights & Evaluate"
      ],
      "metadata": {
        "id": "fxrywnz1i1fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### YOUR CODE HERE #####\n",
        "\n",
        "# initialize a *new*, untrained network object\n",
        "# load our trained weights\n",
        "# put network on GPU (if it is available)\n",
        "\n",
        "#### END OF CODE #####\n",
        "\n",
        "network_load.eval()"
      ],
      "metadata": {
        "id": "BMGXqb47i2_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate our network to make sure we have loaded the weights properly :)\n",
        "test_loss, test_accuracy = evaluate(test_loader, network_load, criterion)\n",
        "print(f\"Test accuracy: {str(test_accuracy * 100):.6}%   Test loss: {str(test_loss):.6}\")"
      ],
      "metadata": {
        "id": "fYWQqx_ajDUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying PyTorch Models\n",
        "\n",
        "Deploying a PyTorch model into your web app is usually not the easiest thing. For example, you would want a dedicated DevOps person in your Hackathon group where all their job is is figuring out how to do this :P\n",
        "\n",
        "PyTorch themselves have a dencent tutorial on this: https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html\n",
        "\n",
        "TLDR, you will need to save your weights and write a python script (i.e., `.py`) file that\n",
        "\n",
        "1. takes some sort of input as a command-line argument, e.g., the path to the image, and/or takes some sort of webrequest call, and\n",
        "2. with the saved weights, pass the input through your model to obtain and output, and return it somehow, e.g., via `stdout` or another webrequest.\n",
        "\n",
        "In other words, in your `.py` file you will need your class definition of `CNN` and use the code above to load your weights.\n",
        "\n",
        "You will probably want to use a `Flask` backend to do this as `Flask` is written in Python so everything works together well. However, it's fine if your backend is something else as long as you have some way to\n",
        "\n",
        "1. invoke the `.py` model script (e.g., on VM-based servers via command-line arguments) and\n",
        "2. receive the model output.\n",
        "\n",
        "However, using webrequests is usually the preferred route, as the flexibility of which scripts can call and recieve outputs from other scripts is a lot better."
      ],
      "metadata": {
        "id": "YGtSE8Z9H71l"
      }
    }
  ]
}